{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import models, transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Apply transformations\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        label = self.labels[idx]\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(data_dir):\n",
    "    classes = sorted(os.listdir(data_dir))\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    for idx, cls in enumerate(classes):\n",
    "        cls_dir = os.path.join(data_dir, cls)\n",
    "        for img_name in os.listdir(cls_dir):\n",
    "            if img_name.endswith(('.png', '.jpg', '.jpeg')):\n",
    "                image_paths.append(os.path.join(cls_dir, img_name))\n",
    "                labels.append(idx)\n",
    "    return image_paths, labels\n",
    "\n",
    "# Example usage\n",
    "data_dir = 'dataset/'  # Replace with your dataset path\n",
    "image_paths, labels = load_dataset(data_dir)\n",
    "\n",
    "# Split into training and validation sets\n",
    "train_paths, val_paths, train_labels, val_labels = train_test_split(\n",
    "    image_paths, labels, test_size=0.2, random_state=42, stratify=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # Using ImageNet means\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, num_classes=4):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.model = models.resnet18(pretrained=True)\n",
    "        num_features = self.model.fc.in_features\n",
    "        self.model.fc = nn.Linear(num_features, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Classifier(num_classes=4).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} - Training'):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f'Training Loss: {epoch_loss:.4f}')\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader, desc=f'Epoch {epoch+1}/{num_epochs} - Validation'):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * images.size(0)\n",
    "            \n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    \n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    accuracy = correct / total\n",
    "    print(f'Validation Loss: {val_loss:.4f} - Accuracy: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'classifier.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "\n",
    "# Initialize GradCAM\n",
    "target_layer = model.model.layer4  # Last convolutional layer in ResNet-18\n",
    "cam = GradCAM(model=model.model, target_layers=[target_layer], use_cuda=torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cam(image_path, model, cam, device, class_idx):\n",
    "    # Load and preprocess image\n",
    "    image = cv2.imread(image_path)\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    input_tensor = transform(image_rgb).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Define target for CAM\n",
    "    target = ClassifierOutputTarget(class_idx)\n",
    "    \n",
    "    # Generate CAM\n",
    "    grayscale_cam = cam(input_tensor=input_tensor, targets=[target])\n",
    "    grayscale_cam = grayscale_cam[0, :]\n",
    "    \n",
    "    # Overlay CAM on image\n",
    "    cam_image = show_cam_on_image(image_rgb / 255.0, grayscale_cam, use_rgb=True)\n",
    "    \n",
    "    return cam_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Generate CAM for the first image in validation set\n",
    "example_img_path = val_paths[0]\n",
    "example_label = val_labels[0]\n",
    "\n",
    "cam_image = generate_cam(example_img_path, model, cam, device, example_label)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(cam_image)\n",
    "plt.title(f'CAM for Class {example_label}')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pseudo_mask(image_path, model, cam, device, num_classes=4, threshold=0.2):\n",
    "    # Initialize an empty mask\n",
    "    mask = np.zeros((224, 224), dtype=np.uint8)\n",
    "    \n",
    "    # Iterate over each class to generate CAMs\n",
    "    for cls in range(num_classes):\n",
    "        cam_image = generate_cam(image_path, model, cam, device, cls)\n",
    "        cam_gray = cv2.cvtColor(cam_image, cv2.COLOR_RGB2GRAY)\n",
    "        _, cam_binary = cv2.threshold(cam_gray, int(threshold * 255), 255, cv2.THRESH_BINARY)\n",
    "        \n",
    "        # Assign class label where CAM is active\n",
    "        mask[cam_binary > 0] = cls\n",
    "    \n",
    "    return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_masks = {}\n",
    "for img_path in tqdm(val_paths, desc='Generating Pseudo-Masks'):\n",
    "    mask = create_pseudo_mask(img_path, model, cam, device, num_classes=4, threshold=0.2)\n",
    "    pseudo_masks[img_path] = mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_mask = pseudo_masks[example_img_path]\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(cv2.cvtColor(cv2.imread(example_img_path), cv2.COLOR_BGR2RGB))\n",
    "plt.title('Original Image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(example_mask, cmap='jet', alpha=0.5)\n",
    "plt.title('Pseudo-Mask')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, num_classes=4, in_channels=3, dropout_rate=0.5):\n",
    "        super(UNet, self).__init__()\n",
    "        def CBR(in_ch, out_ch):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "                nn.BatchNorm2d(out_ch),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            )\n",
    "        \n",
    "        # Encoder\n",
    "        self.enc1 = CBR(in_channels, 64)\n",
    "        self.enc2 = CBR(64, 128)\n",
    "        self.enc3 = CBR(128, 256)\n",
    "        self.enc4 = CBR(256, 512)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.bottleneck = CBR(512, 1024)\n",
    "        \n",
    "        # Decoder\n",
    "        self.upconv4 = nn.ConvTranspose2d(1024, 512, 2, stride=2)\n",
    "        self.dec4 = CBR(1024, 512)\n",
    "        \n",
    "        self.upconv3 = nn.ConvTranspose2d(512, 256, 2, stride=2)\n",
    "        self.dec3 = CBR(512, 256)\n",
    "        \n",
    "        self.upconv2 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n",
    "        self.dec2 = CBR(256, 128)\n",
    "        \n",
    "        self.upconv1 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
    "        self.dec1 = CBR(128, 64)\n",
    "        \n",
    "        # Final Convolution\n",
    "        self.conv_final = nn.Conv2d(64, num_classes, kernel_size=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        e1 = self.enc1(x)\n",
    "        p1 = self.pool(e1)\n",
    "        \n",
    "        e2 = self.enc2(p1)\n",
    "        p2 = self.pool(e2)\n",
    "        \n",
    "        e3 = self.enc3(p2)\n",
    "        p3 = self.pool(e3)\n",
    "        \n",
    "        e4 = self.enc4(p3)\n",
    "        p4 = self.pool(e4)\n",
    "        \n",
    "        # Bottleneck\n",
    "        b = self.bottleneck(p4)\n",
    "        \n",
    "        # Decoder\n",
    "        up4 = self.upconv4(b)\n",
    "        up4 = torch.cat([up4, e4], dim=1)\n",
    "        d4 = self.dec4(up4)\n",
    "        \n",
    "        up3 = self.upconv3(d4)\n",
    "        up3 = torch.cat([up3, e3], dim=1)\n",
    "        d3 = self.dec3(up3)\n",
    "        \n",
    "        up2 = self.upconv2(d3)\n",
    "        up2 = torch.cat([up2, e2], dim=1)\n",
    "        d2 = self.dec2(up2)\n",
    "        \n",
    "        up1 = self.upconv1(d2)\n",
    "        up1 = torch.cat([up1, e1], dim=1)\n",
    "        d1 = self.dec1(up1)\n",
    "        \n",
    "        out = self.conv_final(d1)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, image_paths, masks, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.masks = masks\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Load mask\n",
    "        mask = self.masks[img_path]\n",
    "        \n",
    "        # Apply transformations\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "        else:\n",
    "            # Basic transformations\n",
    "            transform_ops = transforms.Compose([\n",
    "                transforms.ToPILImage(),\n",
    "                transforms.Resize((224, 224)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "            ])\n",
    "            image = transform_ops(image)\n",
    "            mask = torch.from_numpy(mask).long()\n",
    "        \n",
    "        return image, mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install albumentations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "def get_segmentation_transforms():\n",
    "    return A.Compose([\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.Rotate(limit=15, p=0.5),\n",
    "        A.RandomBrightnessContrast(p=0.5),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                    std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2()\n",
    "    ], additional_targets={'mask': 'mask'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations\n",
    "seg_transforms = get_segmentation_transforms()\n",
    "\n",
    "# Create dataset\n",
    "unet_dataset = SegmentationDataset(val_paths, pseudo_masks, transform=seg_transforms)\n",
    "\n",
    "# Split into training and validation (optional)\n",
    "train_unet, val_unet = train_test_split(unet_dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create DataLoaders\n",
    "unet_train_loader = DataLoader(train_unet, batch_size=16, shuffle=True, num_workers=4)\n",
    "unet_val_loader = DataLoader(val_unet, batch_size=16, shuffle=False, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        # inputs: (N, C, H, W)\n",
    "        # targets: (N, H, W)\n",
    "        inputs = torch.softmax(inputs, dim=1)\n",
    "        targets_one_hot = nn.functional.one_hot(targets, num_classes=inputs.shape[1]).permute(0, 3, 1, 2).float()\n",
    "        \n",
    "        intersection = (inputs * targets_one_hot).sum(dim=(2,3))\n",
    "        dice = (2. * intersection + self.smooth) / (inputs.sum(dim=(2,3)) + targets_one_hot.sum(dim=(2,3)) + self.smooth)\n",
    "        return 1 - dice.mean()\n",
    "    \n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, weight=None):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.ce = nn.CrossEntropyLoss(weight=weight)\n",
    "        self.dice = DiceLoss()\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = self.ce(inputs, targets)\n",
    "        dice_loss = self.dice(inputs, targets)\n",
    "        return ce_loss + dice_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 4\n",
    "unet_model = UNet(num_classes=num_classes, in_channels=3, dropout_rate=0.5).to(device)\n",
    "criterion = CombinedLoss()\n",
    "optimizer = optim.Adam(unet_model.parameters(), lr=1e-4)\n",
    "num_epochs = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    unet_model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, masks in tqdm(unet_train_loader, desc=f'UNet Epoch {epoch+1}/{num_epochs} - Training'):\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = unet_model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / len(unet_train_loader.dataset)\n",
    "    print(f'UNet Training Loss: {epoch_loss:.4f}')\n",
    "    \n",
    "    # Validation\n",
    "    unet_model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, masks in tqdm(unet_val_loader, desc=f'UNet Epoch {epoch+1}/{num_epochs} - Validation'):\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            \n",
    "            outputs = unet_model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            \n",
    "            val_loss += loss.item() * images.size(0)\n",
    "    \n",
    "    val_loss /= len(unet_val_loader.dataset)\n",
    "    print(f'UNet Validation Loss: {val_loss:.4f}')\n",
    "    \n",
    "    # Early Stopping or Checkpointing can be added here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(unet_model.state_dict(), 'unet.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_segmentation(model, image_path, device):\n",
    "    model.eval()\n",
    "    \n",
    "    # Load and preprocess image\n",
    "    image = cv2.imread(image_path)\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    transform_ops = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    input_tensor = transform_ops(image_rgb).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        preds = torch.argmax(output, dim=1).squeeze(0).cpu().numpy()\n",
    "    \n",
    "    # Resize mask to original image size\n",
    "    mask = cv2.resize(preds.astype(np.uint8), (image.shape[1], image.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "    return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_image_path = 'path_to_new_image.jpg'  # Replace with your image path\n",
    "predicted_mask = predict_segmentation(unet_model, new_image_path, device)\n",
    "\n",
    "# Visualize\n",
    "original_image = cv2.cvtColor(cv2.imread(new_image_path), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(original_image)\n",
    "plt.title('Original Image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(predicted_mask, cmap='jet', alpha=0.5)\n",
    "plt.title('Predicted Segmentation Mask')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
